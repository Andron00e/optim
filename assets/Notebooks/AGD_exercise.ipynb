{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Binary logistic regression and accelerated methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this problem we will work with accelerated methods applied to the logistic regression problem. A good visual introduction to the topic is available [here](https://mlu-explain.github.io/logistic-regression/). \n",
        "    \n",
        "Logistic regression is a standard model in classification tasks. For simplicity, consider only the case of binary classification. Informally, the problem is formulated as follows: There is a training sample $\\{(a_i, b_i)\\}_{i=1}^m$, consisting of $m$ vectors $a_i \\in \\mathbb{R}^n$ (referred to as features) and corresponding numbers $b_i \\in \\{-1, 1\\}$ (referred to as classes or labels). The goal is to construct an algorithm $b(\\cdot)$, which for any new feature vector $a$ automatically determines its class $b(a) \\in \\{-1, 1\\}$. \n",
        "\n",
        "In the logistic regression model, the class determination is performed based on the sign of the linear combination of the components of the vector $a$ with some fixed coefficients $x \\in \\mathbb{R}^n$:\n",
        "\n",
        "$$\n",
        "b(a) := \\text{sign}(\\langle a, x \\rangle).\n",
        "$$\n",
        "\n",
        "The coefficients $x$ are the parameters of the model and are adjusted by solving the following optimization problem:\n",
        "\n",
        "$$\n",
        "\\tag{LogReg}\n",
        "\\min_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{m} \\sum_{i=1}^m \\ln(1 + \\exp(-b_i \\langle a_i, x \\rangle)) + \\frac{\\lambda}{2} \\|x\\|^2 \\right),\n",
        "$$\n",
        "\n",
        "where $\\lambda \\geq 0$ is the regularization coefficient (a model parameter). \n",
        "\n",
        "1. Will the LogReg problem be convex for $\\lambda = 0$? What is the gradient of the objective function? Will it be strongly convex? What if you will add regularization with $\\lambda > 0$?\n",
        "1. We will work with the real-world data for $A$ and $b$: take the mushroom dataset. Be careful, you will need to predict if the mushroom is poisonous or edible. A poor model can cause death in this exercise. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully.\n",
            "Number of samples: 8124, Number of features: 112\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "# URL of the file to download\n",
        "url = 'https://mipt23.fmin.xyz/files/mushrooms.txt'\n",
        "\n",
        "# Download the file and save it locally\n",
        "response = requests.get(url)\n",
        "dataset = 'mushrooms.txt'\n",
        "\n",
        "# Ensure the request was successful\n",
        "if response.status_code == 200:\n",
        "    with open(dataset, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Load the dataset from the downloaded file\n",
        "    data = load_svmlight_file(dataset)\n",
        "    A, b = data[0].toarray(), data[1]\n",
        "    n, d = A.shape\n",
        "\n",
        "    print(\"Data loaded successfully.\")\n",
        "    print(f\"Number of samples: {n}, Number of features: {d}\")\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Divide the data into two parts: training and test. We will train the model on the $A_{train}$, $b_{train}$ and measure the accuracy of the model on the $A_{test}$, $b_{test}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into training and test sets\n",
        "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=214)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. For the training part $A_{train}$, $b_{train}$, estimate the constants $\\mu, L$ of the training/optimization problem. Use the same small value $\\lambda$ for all experiments\n",
        "1. Using gradient descent with the step $\\frac{1}{L}$, train a model. Plot: accuracy versus iteration number. \n",
        "\n",
        "    $$\n",
        "    \\tag{HB}\n",
        "    x_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta (x_k - x_{k-1})\n",
        "    $$\n",
        "\n",
        "    Fix a step $\\alpha = \\frac{1}{L}$ and search for different values of the momentum $\\beta$ from $-1$ to $1$. Choose your own convergence criterion and plot convergence for several values of momentum on the same graph. Is the convergence always monotonic?\n",
        "\n",
        "1. For the best value of momentum $\\beta$, plot the dependence of the model accuracy on the test sample on the running time of the method. Add to the same graph the convergence of gradient descent with step $\\frac{1}{L}$. Draw a conclusion. Ensure, that you use the same value of $\\lambda$ for both methods.\n",
        "1. Solve the logistic regression problem using the Nesterov method. \n",
        "\n",
        "    $$\n",
        "    \\tag{NAG}\n",
        "    x_{k+1} = x_k - \\alpha \\nabla f(x_k + \\beta (x_k - x_{k-1})) + \\beta (x_k - x_{k-1})  \n",
        "    $$\n",
        "\n",
        "    Fix a step $\\frac{1}{L}$ and search for different values of momentum $\\beta$ from $-1$ to $1$. Check also the momentum values equal to $\\frac{k}{k+3}$, $\\frac{k}{k+2}$, $\\frac{k}{k+1}$ ($k$ is the number of iterations), and if you are solving a strongly convex problem, also $\\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}$. Plot the convergence of the method as a function of the number of iterations (choose the convergence criterion yourself) for different values of the momentum. Is the convergence always monotonic?\n",
        "1. For the best value of momentum $\\beta$, plot the dependence of the model accuracy on the test sample on the running time of the method. Add this graph to the graphs for the heavy ball and gradient descent from the previous steps. Make a conclusion.\n",
        "1. Now we drop the estimated value of $L$ and will try to do it adaptively. Let us make the selection of the constant $L$ adaptive. \n",
        "\n",
        "    $$\n",
        "    f(y) \\leq f(x^k) + \\langle \\nabla f(x^k), y - x^k \\rangle + \\frac{L}{2}\\|x^k - y\\|_2^2\n",
        "    $$\n",
        "    \n",
        "    In particular, the procedure might work:\n",
        "\n",
        "    ```python\n",
        "    def backtracking_L(f, grad, x, h, L0, rho, maxiter=100):\n",
        "        L = L0\n",
        "        fx = f(x)\n",
        "        gradx = grad(x)\n",
        "        iter = 0\n",
        "        while iter < maxiter :\n",
        "            y = x - 1 / L * h\n",
        "            if f(y) <= fx - 1 / L gradx.dot(h) + 1 / (2 * L) h.dot(h):\n",
        "                break\n",
        "            else:\n",
        "                L = L * rho\n",
        "            \n",
        "            iter += 1\n",
        "        return L\n",
        "    ```\n",
        "\n",
        "    What should $h$ be taken as? Should $\\rho$ be greater or less than $1$? Should $L_0$ be taken as large or small? Draw a similar figure as it was in the previous step for L computed adaptively (6 lines - GD, HB, NAG, GD adaptive L, HB adaptive L, NAG adaptive L)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
